{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b72925d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "variables.h5                                   2023-04-17 14:31:04       484040\n",
      "config.json                                    2023-04-17 14:31:04         3546\n",
      "metadata.json                                  2023-04-17 14:31:04           64\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......mean_metric_wrapper\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........17\n",
      ".........18\n",
      ".........19\n",
      ".........2\n",
      ".........20\n",
      ".........21\n",
      ".........22\n",
      ".........23\n",
      ".........24\n",
      ".........25\n",
      ".........26\n",
      ".........27\n",
      ".........28\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pefile\n",
    "import os\n",
    "import hashlib\n",
    "import array\n",
    "import math\n",
    "import csv\n",
    "\n",
    "\n",
    "model=pickle.load(open('model8.pkl','rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fca3695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(data):\n",
    "    if len(data) == 0:\n",
    "        return 0.0\n",
    "    occurences = array.array('L', [0]*256)\n",
    "    for x in data:\n",
    "        occurences[x if isinstance(x, int) else ord(x)] += 1\n",
    "\n",
    "    entropy = 0\n",
    "    for x in occurences:\n",
    "        if x:\n",
    "            p_x = float(x) / len(data)\n",
    "            entropy -= p_x*math.log(p_x, 2)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "def get_resources(pe):\n",
    "    \"\"\"Extract resources :\n",
    "    [entropy, size]\"\"\"\n",
    "    resources = []\n",
    "    if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE'):\n",
    "        try:\n",
    "            for resource_type in pe.DIRECTORY_ENTRY_RESOURCE.entries:\n",
    "                if hasattr(resource_type, 'directory'):\n",
    "                    for resource_id in resource_type.directory.entries:\n",
    "                        if hasattr(resource_id, 'directory'):\n",
    "                            for resource_lang in resource_id.directory.entries:\n",
    "                                data = pe.get_data(resource_lang.data.struct.OffsetToData, resource_lang.data.struct.Size)\n",
    "                                size = resource_lang.data.struct.Size\n",
    "                                entropy = get_entropy(data)\n",
    "\n",
    "                                resources.append([entropy, size])\n",
    "        except Exception as e:\n",
    "            return resources\n",
    "    return resources\n",
    "\n",
    "def get_version_info(pe):\n",
    "    \"\"\"Return version infos\"\"\"\n",
    "    res = {}\n",
    "    for fileinfo in pe.FileInfo:\n",
    "        if fileinfo.Key == 'StringFileInfo':\n",
    "            for st in fileinfo.StringTable:\n",
    "                for entry in st.entries.items():\n",
    "                    res[entry[0]] = entry[1]\n",
    "        if fileinfo.Key == 'VarFileInfo':\n",
    "            for var in fileinfo.Var:\n",
    "                res[var.entry.items()[0][0]] = var.entry.items()[0][1]\n",
    "    if hasattr(pe, 'VS_FIXEDFILEINFO'):\n",
    "        res['flags'] = pe.VS_FIXEDFILEINFO.FileFlags\n",
    "        res['os'] = pe.VS_FIXEDFILEINFO.FileOS\n",
    "        res['type'] = pe.VS_FIXEDFILEINFO.FileType\n",
    "        res['file_version'] = pe.VS_FIXEDFILEINFO.FileVersionLS\n",
    "        res['product_version'] = pe.VS_FIXEDFILEINFO.ProductVersionLS\n",
    "        res['signature'] = pe.VS_FIXEDFILEINFO.Signature\n",
    "        res['struct_version'] = pe.VS_FIXEDFILEINFO.StrucVersion\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6742464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path):\n",
    "    \n",
    "        # Load the PE file\n",
    "        pe = pefile.PE(file_path)\n",
    "        entropy = list(map(lambda x:x.get_entropy(), pe.sections))\n",
    "        raw_sizes = list(map(lambda x:x.SizeOfRawData, pe.sections))\n",
    "        virtual_sizes = list(map(lambda x:x.Misc_VirtualSize, pe.sections))\n",
    "        \n",
    "        # Extract features from the PE file\n",
    "        features = {\n",
    "            \n",
    "            \"Machine\": pe.FILE_HEADER.Machine,\n",
    "            \"SizeOfOptionalHeader\": pe.FILE_HEADER.SizeOfOptionalHeader,\n",
    "            \"Characteristics\": pe.FILE_HEADER.Characteristics,\n",
    "            \"MajorLinkerVersion\": pe.OPTIONAL_HEADER.MajorLinkerVersion,\n",
    "            \"MinorLinkerVersion\": pe.OPTIONAL_HEADER.MinorLinkerVersion,\n",
    "            \"SizeOfCode\": pe.OPTIONAL_HEADER.SizeOfCode,\n",
    "            \"SizeOfInitializedData\": pe.OPTIONAL_HEADER.SizeOfInitializedData,\n",
    "            \"SizeOfUninitializedData\": pe.OPTIONAL_HEADER.SizeOfUninitializedData,\n",
    "            \"AddressOfEntryPoint\": pe.OPTIONAL_HEADER.AddressOfEntryPoint,\n",
    "            \"BaseOfCode\": pe.OPTIONAL_HEADER.BaseOfCode,\n",
    "            \"BaseOfData\": pe.OPTIONAL_HEADER.BaseOfData,\n",
    "            \"ImageBase\":pe.OPTIONAL_HEADER.ImageBase,\n",
    "            \"SectionAlignment\":pe.OPTIONAL_HEADER.SectionAlignment,\n",
    "            \"FileAlignment\":pe.OPTIONAL_HEADER.FileAlignment,\n",
    "            \"MajorOperatingSystemVersion\":pe.OPTIONAL_HEADER.MajorOperatingSystemVersion,\n",
    "            \"MinorOperatingSystemVersion\":pe.OPTIONAL_HEADER.MinorOperatingSystemVersion,\n",
    "            \"MajorImageVersion\":pe.OPTIONAL_HEADER.MajorImageVersion,\n",
    "            \"MinorImageVersion\":pe.OPTIONAL_HEADER.MinorImageVersion,\n",
    "            \"MajorSubsystemVersion\":pe.OPTIONAL_HEADER.MajorSubsystemVersion,\n",
    "            \"MinorSubsystemVersion\":pe.OPTIONAL_HEADER.MinorSubsystemVersion,\n",
    "            \"SizeOfImage\":pe.OPTIONAL_HEADER.SizeOfImage,\n",
    "            \"SizeOfHeaders\": pe.OPTIONAL_HEADER.SizeOfHeaders,\n",
    "            \"CheckSum\": pe.OPTIONAL_HEADER.CheckSum,\n",
    "            \"Subsystem\": pe.OPTIONAL_HEADER.Subsystem,\n",
    "            \"DllCharacteristics\": pe.OPTIONAL_HEADER.DllCharacteristics,\n",
    "            \"SizeOfStackReserve\": pe.OPTIONAL_HEADER.SizeOfStackReserve,\n",
    "            \"SizeOfStackCommit\": pe.OPTIONAL_HEADER.SizeOfStackCommit,\n",
    "            \"SizeOfHeapReserve\": pe.OPTIONAL_HEADER.SizeOfHeapReserve,\n",
    "            \"SizeOfHeapCommit\": pe.OPTIONAL_HEADER.SizeOfHeapCommit,\n",
    "            \"LoaderFlags\": pe.OPTIONAL_HEADER.LoaderFlags,\n",
    "            \"NumberOfRvaAndSizes\": pe.OPTIONAL_HEADER.NumberOfRvaAndSizes,\n",
    "            \"SectionsNb\":len(pe.sections)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            features.update({\"SectionsMeanEntropy\":float(sum(entropy))/len(entropy)})\n",
    "            \n",
    "        except:\n",
    "            features.update({\"SectionsMeanEntropy\":0})\n",
    "            \n",
    "        features.update({\n",
    "            \"SectionsMinEntropy\":min(entropy),\n",
    "            \"SectionsMaxEntropy\":max(entropy)\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            features.update({\"SectionsMeanRawsize\":float(sum(raw_sizes))/len(raw_sizes)})\n",
    "            \n",
    "        except:\n",
    "            features.update({\"SectionsMeanRawsize\":0})\n",
    "            \n",
    "        features.update({\n",
    "            \"SectionsMinRawsize\":min(raw_sizes),\n",
    "            \"SectionMaxRawsize\":max(raw_sizes)\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            features.update({\"SectionsMeanVirtualsize\": float(sum(virtual_sizes))/len(virtual_sizes)})\n",
    "        except:\n",
    "            features.update({\"SectionsMeanVirtualsize\":0})\n",
    "            \n",
    "        features.update({\n",
    "            \"SectionsMinVirtualsize\":min(virtual_sizes),\n",
    "            \"SectionMaxVirtualsize\":max(virtual_sizes)\n",
    "        })\n",
    "        \n",
    "        \n",
    "    #Imports\n",
    "        try:\n",
    "            imports = sum([x.imports for x in pe.DIRECTORY_ENTRY_IMPORT], [])\n",
    "        \n",
    "            features.update({\n",
    "                \"ImportsNbDLL\":len(pe.DIRECTORY_ENTRY_IMPORT),\n",
    "                \"ImportsNb\":len(imports),\n",
    "                \"ImportsNbOrdinal\":len(list(filter(lambda x:x.name is None, imports)))\n",
    "            })\n",
    "        except AttributeError:\n",
    "            features.update({\n",
    "                \"ImportsNbDLL\":0,\n",
    "                \"ImportsNb\":0,\n",
    "                \"ImportsNbOrdinal\":0   \n",
    "            })\n",
    "        #Exports\n",
    "        try:\n",
    "            features.update({\"ExportNb\":len(pe.DIRECTORY_ENTRY_EXPORT.symbols)})\n",
    "        except AttributeError:\n",
    "            # No export\n",
    "            features.update({\"ExportNb\":0})\n",
    "            \n",
    "        #Resources\n",
    "        resources= get_resources(pe)\n",
    "        features.update({\"ResourcesNb\":len(resources)})\n",
    "        if len(resources)> 0:\n",
    "            entropy = list(map(lambda x:x[0], resources))\n",
    "            features.update({\n",
    "                \"ResourcesMeanEntropy\":0 if len(entropy)==0 else float(sum(entropy))/len(entropy),\n",
    "                \"ResourcesMinEntropy\":min(entropy),\n",
    "                \"ResourcesMaxEntropy\":max(entropy)\n",
    "            })\n",
    "            sizes = list(map(lambda x:x[1], resources))\n",
    "            features.update({\n",
    "                \"ResourcesMeanSize\":0 if len(sizes)==0 else float(sum(sizes))/len(sizes),\n",
    "                \"ResourcesMinSize\":min(sizes),\n",
    "                \"ResourcesMaxSize\":max(sizes)\n",
    "                })\n",
    "        else:\n",
    "            features.update({\"ResourcesMeanEntropy\":0,\n",
    "                             \"ResourcesMinEntropy\":0,\n",
    "                            \"ResourcesMaxEntropy:\":0,\n",
    "                            \"ResourcesMeanSize\":0,\n",
    "                            \"ResourcesMinSize\":0,\n",
    "                            \"ResourcesMaxSize\":0\n",
    "                            })\n",
    "            \n",
    "         # Load configuration size\n",
    "        try:\n",
    "            features.update({\"LoadConfigurationSize\":pe.DIRECTORY_ENTRY_LOAD_CONFIG.struct.Size})\n",
    "        except AttributeError:\n",
    "            features.update({'LoadConfigurationSize':0})\n",
    "                            \n",
    "                            \n",
    "         # Version configuration size\n",
    "        try:\n",
    "            version_infos = get_version_info(pe)\n",
    "            features.update({'VersionInformationSize':len(version_infos.keys())})\n",
    "        except AttributeError:\n",
    "            features.update({'VersionInformationSize':0})\n",
    "\n",
    "        return features\n",
    "    \n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7db74849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted from /home/vishwanath/Desktop/fyp/7z2201-x64.exe and written to pe_fea.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing the PE file to be analyzed\n",
    "dir_path = r\"/home/vishwanath/Desktop/fyp\"\n",
    "\n",
    "# Define the output CSV file name and path\n",
    "output_file = \"pe_fea.csv\"\n",
    "\n",
    "# Extract features from the PE file and write them to the output CSV file\n",
    "file_path = os.path.join(dir_path, \"7z2201-x64.exe\")\n",
    "features = extract_features(file_path)\n",
    "\n",
    "with open(output_file, mode='w', newline='') as csv_file:\n",
    "    fieldnames = ['Machine','SizeOfOptionalHeader','Characteristics','MajorLinkerVersion','MinorLinkerVersion','SizeOfCode','SizeOfInitializedData','SizeOfUninitializedData','AddressOfEntryPoint','BaseOfCode','BaseOfData','ImageBase','SectionAlignment','FileAlignment','MajorOperatingSystemVersion','MinorOperatingSystemVersion','MajorImageVersion','MinorImageVersion','MajorSubsystemVersion','MinorSubsystemVersion','SizeOfImage','SizeOfHeaders','CheckSum','Subsystem','DllCharacteristics','SizeOfStackReserve','SizeOfStackCommit','SizeOfHeapReserve','SizeOfHeapCommit','LoaderFlags','NumberOfRvaAndSizes','SectionsNb','SectionsMeanEntropy','SectionsMinEntropy','SectionsMaxEntropy','SectionsMeanRawsize','SectionsMinRawsize','SectionMaxRawsize','SectionsMeanVirtualsize','SectionsMinVirtualsize','SectionMaxVirtualsize','ImportsNbDLL','ImportsNb','ImportsNbOrdinal','ExportNb','ResourcesNb','ResourcesMeanEntropy','ResourcesMinEntropy','ResourcesMaxEntropy','ResourcesMeanSize','ResourcesMinSize','ResourcesMaxSize','LoadConfigurationSize','VersionInformationSize']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    if features:\n",
    "        writer.writerow(features)\n",
    "        print(f\"Features extracted from {file_path} and written to {output_file}\")\n",
    "    else:\n",
    "        print(f\"Features not extracted from {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2879ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('pe_fea.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7be3bc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.32000000e+02, 2.24000000e+02, 3.03000000e+02, 6.00000000e+00,\n",
       "        0.00000000e+00, 2.66240000e+04, 1.99680000e+04, 0.00000000e+00,\n",
       "        3.01000000e+04, 4.09600000e+03, 3.27680000e+04, 4.19430400e+06,\n",
       "        4.09600000e+03, 5.12000000e+02, 4.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.00000000e+00, 0.00000000e+00,\n",
       "        5.73440000e+04, 1.02400000e+03, 0.00000000e+00, 2.00000000e+00,\n",
       "        2.56000000e+02, 1.04857600e+06, 4.09600000e+03, 1.04857600e+06,\n",
       "        4.09600000e+03, 0.00000000e+00, 1.60000000e+01, 4.00000000e+00,\n",
       "        3.91927550e+00, 2.03931352e-02, 6.59275759e+00, 9.08800000e+03,\n",
       "        5.12000000e+02, 2.66240000e+04, 1.14880000e+04, 4.07200000e+03,\n",
       "        2.64120000e+04, 6.00000000e+00, 7.70000000e+01, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.00000000e+00, 2.90770595e+00, 1.39918346e+00,\n",
       "        5.38456361e+00, 6.04333333e+02, 3.40000000e+01, 1.45800000e+03,\n",
       "        0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=data.values\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "805105cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fedaffeb7f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 108ms/step\n"
     ]
    }
   ],
   "source": [
    "res=model.predict(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57ae8331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f01d22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
